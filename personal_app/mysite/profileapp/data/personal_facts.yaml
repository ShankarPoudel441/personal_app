qa:
  - q: "What is your full name?"
    a: "My name is Shankar Poudel."

  - q: "What is your educational background?"
    a: "I completed my M.S. in Computer Science and Engineering at the University of Nevada, Reno, and my B.E. in Electronics and Communication Engineering at the Institute of Engineering (Tribhuvan University), Nepal."

  - q: "Where did you do your Master's?"
    a: "I did my Master of Science in Computer Science and Engineering at the University of Nevada, Reno, from 2021 to 2023, graduating with a 4.0 GPA."

  - q: "Where did you do your Bachelor's?"
    a: "I completed my Bachelor of Electronics and Communication Engineering at the Institute of Engineering (IOE), Tribhuvan University in Nepal, with a GPA of 3.7."

  - q: "What is your current role?"
    a: "I am working as a Computer Engineer at G4-Tech in the United States."

  - q: "What do you work on at G4-Tech?"
    a: "At G4-Tech, I work on real-time Intelligent Compaction tools in Azure Cloud. I design and deploy CI/CD pipelines, build and maintain ML models that predict compaction density within about one percent of the true value, and design both hardware and software for field-deployed IC systems."

  - q: "What was your role as a Research Assistant?"
    a: "As a Research Assistant at the University of Nevada, Reno, I developed an Intelligent Compaction tool using vibration signal analysis and unsupervised learning to predict asphalt density. We achieved density prediction accuracy within ±1%."

  - q: "Where have you worked before G4-Tech?"
    a: "Before G4-Tech, I worked as a Research Assistant at UNR, a Machine Learning Engineer at D-Tech Office System in Nepal, and a Software Engineer at Wiseyak Solutions."

  - q: "What did you do at Wiseyak Solutions?"
    a: "At Wiseyak, I helped develop an Electronic Health Record system using FHIR ontology and graph databases, built backend microservices, and created classification models for injury-type prediction using TF-IDF and SVM, plus annotation tools for medical images."

  - q: "What did you do at D-Tech Office System?"
    a: "At D-Tech, I worked as a Machine Learning Engineer, creating and maintaining a content-based recommendation system for online shopping platforms and collaborating on a Nepali text-to-speech model."

  - q: "What are your main technical strengths?"
    a: "My main strengths are in machine learning, intelligent compaction systems, data engineering, and production-grade software. I work a lot with Python, PyTorch, scikit-learn, data pipelines, Azure Cloud, InfluxDB, CosmosDB, and KustoDB."

  - q: "What programming languages do you use?"
    a: "I primarily use Python, but I also have experience with C++, Clojure, and ClojureScript."

  - q: "What tools and libraries do you often work with?"
    a: "I frequently use NumPy, Pandas, Matplotlib, SciPy, scikit-learn, TensorFlow, Keras, and OpenCV. On the data side, I work with SQL, InfluxDB, KustoDB, and graph databases like D-Graph. I also use Docker and Git in my daily workflow."

  - q: "What is RICA?"
    a: "RICA stands for Retrofit Intelligent Compaction Analyzer. It is a retrofit IC system I co-developed that collects live roller vibration and location data, analyzes it in real time, and visualizes predicted asphalt density and compaction quality."

  - q: "What kind of projects have you done besides intelligent compaction?"
    a: "I have worked on a voice-based invoice generation system using MFCC features and CNNs, a transfer-learned RUNet model for cancerous mass detection in mammograms, and a CNN-based OCR system for old Nepali and English printed documents."

  - q: "What are your main research interests?"
    a: "My research interests include intelligent compaction, ML systems for real-world sensing, adversarial robustness and security of AI models, and multimodal large language models."

  - q: "What online certifications do you have?"
    a: "I have completed the Machine Learning course on Coursera, the Deep Learning Specialization on Coursera, and a TensorFlow Developer Certificate course on Udemy."

  - q: "What kind of roles are you looking for?"
    a: "I am interested in full-time roles in software engineering or machine learning engineering, especially positions where I can work on predictive modeling, ML systems, IoT data pipelines, and AI safety."

  - q: "Where are you currently based?"
    a: "I am currently based in Nevada, USA."

  - q: "What is your educational background?"
    a: "I completed my M.S. in Computer Science and Engineering from the University of Nevada, Reno, and a B.E. in Electronics and Communication Engineering from Tribhuvan University, Nepal."

  - q: "What are your main areas of expertise?"
    a: "My expertise spans Machine Learning, Deep Learning, Intelligent Compaction Systems, Transformer-based LLMs, and data engineering for real-world sensing systems."

  - q: "Do you work with LLMs?"
    a: "Yes, I work actively with large language models, transformer architectures, and retrieval-augmented generation systems. I have trained a GPT-2-like model locally and fine-tuned multiple transformer-based models for specialized tasks."

  - q: "What LLM architectures do you have experience with?"
    a: "I have hands-on experience with GPT-style auto-regressive transformers, encoder-only transformers like BERT, encoder-decoder architectures, and multimodal transformer-based systems."

  - q: "Have you trained any LLMs yourself?"
    a: "Yes. I have trained a GPT-2-like decoder transformer model from scratch on a local machine, implementing positional encodings, multi-head attention, feedforward layers, tokenizers, and training loops using PyTorch."

  - q: "Do you understand how multi-head attention works?"
    a: "Yes. I have in-depth understanding of multi-head scaled dot-product attention, including query-key-value projections, attention weights, softmax normalization, context vector construction, and how multiple heads capture diverse subspace relations."

  - q: "What kind of tasks have you fine-tuned LLMs for?"
    a: "I have fine-tuned LLMs for text generation, summarization, domain-specific Q/A, structured extraction, and semantic retrieval tasks."

  - q: "Have you worked with LangChain?"
    a: "Yes. I have used LangChain for Retrieval-Augmented Generation (RAG), vectorstore pipelines, prompt templates, document loaders, and custom tool integrations."

  - q: "Do you have experience creating RAG systems?"
    a: "Yes. I have built RAG pipelines using embeddings, chunking strategies, vector databases, and prompt engineering to provide grounded, context-aware responses."

  - q: "What embeddings do you typically use?"
    a: "For RAG and semantic search, I often use sentence-transformers like all-MiniLM, BERT-based embeddings, or LLM-based embeddings depending on compute and accuracy requirements."

  - q: "What are your ML strengths?"
    a: "Signal processing, predictive modeling, transformer architectures, embedding systems, and production-grade ML deployments."

  - q: "Where do you currently work?"
    a: "I work as a Computer Engineer at G4-Tech, focusing on real-time Intelligent Compaction systems using cloud and machine learning pipelines."

  - q: "What programming languages do you use?"
    a: "I mainly use Python, along with experience in C++, Clojure, ClojureScript, and scripting tools used in ML workflows."

  - q: "What tools and libraries do you frequently use?"
    a: "PyTorch, TensorFlow, Scikit-learn, NumPy, Pandas, LangChain, HuggingFace Transformers, OpenCV, Docker, Azure Cloud, InfluxDB, KustoDB, and CosmosDB."

  - q: "What databases are you familiar with?"
    a: "SQL, InfluxDB, CosmosDB, KustoDB, D-Graph, and vector databases used in RAG systems."

  - q: "What certifications do you have?"
    a: "Machine Learning (Coursera), Deep Learning Specialization (Coursera), and the TensorFlow Developer Certificate (Udemy)."

  - q: "What kind of roles are you looking for?"
    a: "Roles in Machine Learning Engineering, Software Engineering, AI Systems, or Applied LLM research."



facts:
  - "I am a Computer Engineer specializing in Machine Learning, Deep Learning, and Intelligent Compaction systems."
  - "I completed my M.S. in Computer Science and Engineering at the University of Nevada, Reno with a 4.0 GPA."
  - "I earned my Bachelor''s degree in Electronics and Communication Engineering from Tribhuvan University in Nepal with a 3.7 GPA."
  - "I work at G4-Tech in the USA on real-time Intelligent Compaction tools deployed in Azure Cloud."
  - "My work at G4-Tech includes CI/CD pipelines, ML models for asphalt density prediction, and both hardware and software design for field IC systems."
  - "As a Research Assistant at UNR, I helped develop an Intelligent Compaction tool that predicts asphalt density within about ±1%."
  - "I have industrial experience as a Machine Learning Engineer and as a Software Engineer in Nepal."
  - "At D-Tech, I worked on recommender systems for online shopping and on a Nepali text-to-speech system."
  - "At Wiseyak, I worked on FHIR-based Electronic Health Records and graph databases for medical data."
  - "I have built classification models using TF-IDF and SVM for medical injury-type prediction."
  - "I co-developed the Retrofit Intelligent Compaction Analyzer (RICA), a retrofit IC system for asphalt rollers."
  - "I have implemented a voice-based invoice generation system using MFCC and CNN models."
  - "I developed a RUNet-based model to detect and segment cancerous masses in mammograms."
  - "I created a CNN-based OCR pipeline for digitizing old Nepali and English documents into editable text."
  - "I regularly use Python, NumPy, Pandas, scikit-learn, TensorFlow, and PyTorch in my work."
  - "I work with databases such as SQL, InfluxDB, CosmosDB, KustoDB, and D-Graph."
  - "I use Azure Cloud, Docker, and Git for deploying and managing production systems."
  - "I am interested in AI security topics like adversarial robustness, jailbreak detection, and safe deployment of LLMs."
  - "I have hands-on experience building IoT and ML systems that integrate sensors, GPS, and real-time data analytics for construction quality control."
  - "I enjoy research and development at the intersection of machine learning, real-world sensing, and robust, production-ready software systems."
  - "I have trained a GPT-2-like transformer model locally using PyTorch."
  - "I understand transformer internals including attention mechanisms, layer normalization, residual connections, and token embedding strategies."
  - "I have implemented multi-head attention modules from scratch for learning purposes."
  - "I have fine-tuned pretrained LLMs for generation, classification, summarization, and domain-specific Q/A."
  - "I have built complete GPT-style pretraining pipelines including tokenization, batching, positional encoding, and autoregressive loss functions."
  - "I have worked with HuggingFace Transformers for fine-tuning and inference."
  - "I have used LangChain for RAG pipelines, tool calling, and document retrieval workflows."
  - "I have implemented RAG systems using vector databases like FAISS and ChromaDB."
  - "I have experimented with LoRA and QLoRA fine-tuning techniques for parameter-efficient LLM training."
  - "I have trained custom tokenizers using Byte-Pair Encoding."
  - "I have worked with embedding models for semantic search and document retrieval."
  - "I understand scaling laws, context length effects, and transformer performance trade-offs."
  - "I have experience constructing attention masks for autoregressive transformers."
  - "I have built inference pipelines for LLMs including greedy decoding, top-k sampling, and nucleus sampling."
  - "I have designed data pipelines for structured ML tasks and LLM-supported NLP tasks."
  - "I have used PyTorch Lightning for structured training of NLP and ML models."
  - "I have implemented inference-optimized batching and caching strategies for LLM evaluation."
  - "I have explored jailbreak detection techniques and adversarial robustness systems for LLMs."
  - "I have experience with prompt engineering, prompt templates, and context window optimization."
  - "I have studied transformer variants including ALBERT, DistilBERT, GPT-Neo, LLaMA, and encoder-decoder systems."
  - "I have used sentence-transformers extensively for embedding-based retrieval tasks."
  - "I enjoy experimentation with neural sequence models and generative architectures."
  - "I have built multilingual pipelines including Nepali/English OCR using CNNs."
  - "I have developed annotation tools for medical images using ClojureScript."
  - "I co-developed a RUNet-based model for detecting cancerous masses in mammograms."
  - "I have worked on predictive modeling for asphalt compaction quality using vibration and temperature signals."
  - "I have built voice recognition pipelines using MFCC features and CNNs."
  - "I have created real-time IoT and sensor-based ML systems deployed on field hardware."
  - "I work regularly with Azure Cloud, InfluxDB, CosmosDB, and KustoDB for ML and analytics workloads."
  - "I have implemented CI/CD pipelines for cloud-based ML deployments."
  - "I am deeply interested in LLM systems, AI safety, multimodal models, and real-world ML engineering."


